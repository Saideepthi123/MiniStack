{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DQTwme3V9SGZ",
    "outputId": "23351388-fcc2-46c9-9b7f-049a17e2fe5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\SAI\n",
      "[nltk_data]     DEEPTHI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "import threading\n",
    "from nltk.corpus import wordnet\n",
    "import time\n",
    "from multiprocessing import Process, Queue\n",
    "import multiprocessing\n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import csv \n",
    "import requests \n",
    "import xml.etree.ElementTree as ET \n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import random\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "import random\n",
    "from nltk.stem.porter import *\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool, Manager\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EdBRHpGu9kSy",
    "outputId": "3b888224-1df3-4d4a-9242-cc9c03329b21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  XML-Files.zip\n",
      "  inflating: XML-Files/XML-Files/AI_Meta_Posts.xml  \n",
      "  inflating: XML-Files/XML-Files/AI_Posts.xml  \n",
      "  inflating: XML-Files/XML-Files/Computer_graphics_Posts.xml  \n",
      "  inflating: XML-Files/XML-Files/Computergraphics_Meta_Posts.xml  \n",
      "  inflating: XML-Files/XML-Files/CS_Meta_Posts.xml  \n",
      "  inflating: XML-Files/XML-Files/CS_Posts.xml  \n",
      "  inflating: XML-Files/XML-Files/DataScience_Meta_Posts.xml  \n",
      "  inflating: XML-Files/XML-Files/DataScience_Posts.xml  \n"
     ]
    }
   ],
   "source": [
    "!unzip XML-Files.zip -d XML-Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YEBYjv_B-n3R"
   },
   "outputs": [],
   "source": [
    "fields = ['Id', 'Text', 'Topic']\n",
    "\n",
    "def parseXML(xmlfile, start_count): \n",
    "    \"\"\"This function would parse XML file and would extract important information from xml tag 'text' \n",
    "    and returns list containing dict whcih would have text\"\"\"\n",
    "    #https://www.geeksforgeeks.org/xml-parsing-python/\n",
    "    #create element tree object \n",
    "    print(\"File\", xmlfile)\n",
    "    tree = ET.parse(xmlfile) # Creates a parse tree like https://www.google.com/search?q=xml+parse+tree&sxsrf=ALeKk00rw2XjRSAi-qGfaA0Rg4EtJzyx_Q:1585918970675&source=lnms&tbm=isch&sa=X&ved=2ahUKEwi0wY7HqMzoAhVXAXIKHYfdDrkQ_AUoAXoECA4QAw&biw=1366&bih=657#imgrc=P4mJt0kjVEuqVM\n",
    "    topic = xmlfile.split(\"XML-Files\")[1].split(\"_\")[0] # Getting xml file_name from string containing directory/file_name.xml \n",
    "    # get root element using which we would parse xml tree\n",
    "    root = tree.getroot() \n",
    "    # create empty list for news items \n",
    "    newsitems = [] \n",
    "    count = start_count\n",
    "    # iterate news items \n",
    "    for each_row in root.iter(\"row\"):#  iterates only over those elements that have the required tag:\"row\" \n",
    "        news = {}\n",
    "        news[\"Id\"] = count\n",
    "        news[\"Text\"] = each_row.attrib[\"Body\"]\n",
    "        news[\"Topic\"] = topic\n",
    "        count=count+1\n",
    "        newsitems.append(news) #[{id:1,text:\" Some text  \",Topic : \"AI\"},{{id:1,text:\" Some text  \",Topic : \"AI\"},{{id:1,text:\" Some text  \",Topic : \"AI\"},...........]\n",
    "    # return news items list \n",
    "    print(\"len\", len(newsitems))\n",
    "    return newsitems # To save this dict on csv we are putting dict into a list https://www.tutorialspoint.com/How-to-save-a-Python-Dictionary-to-CSV-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DCGPZAtQ-8XY"
   },
   "outputs": [],
   "source": [
    "def savetoCSV(newsitems, filename): \n",
    "    \"\"\"Writes the list of dict returned from above function onto a csv file\"\"\" \n",
    "    # writing to csv file \n",
    "    with open(filename, 'w', encoding = 'utf-8') as csvfile: \n",
    "        # creating a csv dict writer object \n",
    "        writer = csv.DictWriter(csvfile, fieldnames = fields) \n",
    "        # writing headers (field names) \n",
    "        writer.writeheader() \n",
    "        # writing data rows \n",
    "        writer.writerows(newsitems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cHWWqdvR_P7Y",
    "outputId": "bc88f40c-579d-4a37-d271-a681c73830ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AImeta_Posts.xml\n",
      "File XML-Files/AImeta_Posts.xml\n",
      "len 654\n",
      "csv_filename AImeta_Posts.csv\n",
      "AI_Posts.xml\n",
      "File XML-Files/AI_Posts.xml\n",
      "len 16760\n",
      "csv_filename AI_Posts.csv\n",
      "ComputerGraphicsMeta_Posts.xml\n",
      "File XML-Files/ComputerGraphicsMeta_Posts.xml\n",
      "len 299\n",
      "csv_filename ComputerGraphicsMeta_Posts.csv\n",
      "ComputerGraphics_Posts.xml\n",
      "File XML-Files/ComputerGraphics_Posts.xml\n",
      "len 6070\n",
      "csv_filename ComputerGraphics_Posts.csv\n",
      "CSMeta_Posts.xml\n",
      "File XML-Files/CSMeta_Posts.xml\n",
      "len 1585\n",
      "csv_filename CSMeta_Posts.csv\n",
      "CS_Posts.xml\n",
      "File XML-Files/CS_Posts.xml\n",
      "len 81429\n",
      "csv_filename CS_Posts.csv\n",
      "DataScienceMeta_Posts.xml\n",
      "File XML-Files/DataScienceMeta_Posts.xml\n",
      "len 509\n",
      "csv_filename DataScienceMeta_Posts.csv\n",
      "DataScience_Posts.xml\n",
      "File XML-Files/DataScience_Posts.xml\n",
      "len 54869\n",
      "csv_filename DataScience_Posts.csv\n"
     ]
    }
   ],
   "source": [
    "def filterpostfiles(filename):\n",
    "    \"\"\"Returns the name of  xml files present in the directory CML Files \"\"\"\n",
    "    return filename.endswith(\"Posts.xml\") \n",
    "postfiles = filter(filterpostfiles, os.listdir(\"XML-Files\"))\n",
    "\n",
    "# specifying the fields for csv file \n",
    "fields = ['Id', 'Text', 'Topic']  \n",
    "start_count = 0\n",
    "for each_file in postfiles:\n",
    "    print(each_file)\n",
    "    # parse xml file \n",
    "    newsitems = parseXML(\"XML-Files/\"+each_file, start_count)\n",
    "    #Get only name of xml file by sepearting out extension use the same name to save file as .csv \n",
    "    csv_filename = each_file.split('.')[0] + \".csv\"\n",
    "    print(\"csv_filename\", csv_filename)\n",
    "    # store news items in a csv file \n",
    "    savetoCSV(newsitems, \"CSV-Files/\" + csv_filename)\n",
    "    start_count = len(newsitems) + start_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KfpcyNWE_TGU"
   },
   "outputs": [],
   "source": [
    "def filtercsvpostfiles(filename):\n",
    "    \"\"\" Returns all the csv file name extracted from XML which are stored in CSV files\"\"\"\n",
    "    return filename.endswith(\"Posts.csv\") \n",
    "csvpostfiles = filter(filtercsvpostfiles, os.listdir(\"CSV-Files\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LE59DBqkzYMi",
    "outputId": "bd8b9d9a-c809-4d4b-bcda-e4cc543682fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each file AImeta_Posts.csv\n",
      "shape (654, 3)\n",
      "each file AI_Posts.csv\n",
      "shape (16760, 3)\n",
      "each file ComputerGraphicsMeta_Posts.csv\n",
      "shape (299, 3)\n",
      "each file ComputerGraphics_Posts.csv\n",
      "shape (6070, 3)\n",
      "each file CSMeta_Posts.csv\n",
      "shape (1585, 3)\n",
      "each file CS_Posts.csv\n",
      "shape (81429, 3)\n",
      "each file DataScienceMeta_Posts.csv\n",
      "shape (509, 3)\n",
      "each file DataScience_Posts.csv\n",
      "shape (54869, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_dataframe = pd.DataFrame(columns=fields)\n",
    "for each_file in csvpostfiles:\n",
    "    #Merge multiple csv file created from each xml file into just one xml file\n",
    "    print(\"each file\", each_file)\n",
    "    df = pd.read_csv(\"CSV-Files/\"+ each_file)\n",
    "    print(\"shape\", df.shape)\n",
    "    total_dataframe = total_dataframe.append(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RLxsdEjFzi1R",
    "outputId": "52d9d3a9-613b-433d-d2d5-03b43414aba0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of total dataframe (162175, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Shape of total dataframe\", total_dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZruxdnRIzlWb",
    "outputId": "340c61bc-2b50-4f7a-a633-ddeea5a9a410"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of total dataframe after null value remove (161423, 3)\n"
     ]
    }
   ],
   "source": [
    "total_dataframe = total_dataframe[~total_dataframe['Text'].isna()]\n",
    "total_dataframe = total_dataframe[~total_dataframe['Text'].isnull()]\n",
    "print(\"Shape of total dataframe after null value remove\", total_dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "3SasvvvWzneq",
    "outputId": "94e51dc0-f2d7-42e4-afdb-b4250d02c94a"
   },
   "outputs": [],
   "source": [
    "\n",
    "total_dataframe.to_pickle(\"QuestionsDataSet.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "dyHLOEF8zpoW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "DataExtraction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
